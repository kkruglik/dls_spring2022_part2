{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://static.wixstatic.com/media/66c28f_db7a1ba3e35b4b17a6688472c889b7bf~mv2_d_2777_1254_s_2.png/v1/fill/w_710,h_320,al_c,q_85,usm_0.66_1.00_0.01/logo_yellow_white.webp\" width=1000, height=450>\n<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>","metadata":{"id":"tWJTCBu4Tveb"}},{"cell_type":"markdown","source":"# Text Summarization\n\nВсем привет! Сегодня мы познакомимся с задачей суммаризации текста на примере генерации \"сжатых\" новостей. Рассмотрим некоторые базовые решения и познакомимся с архитектурами нейросетей для решения задачи.\nДатасет: gazeta.ru\n\n\n`Ноутбук создан на основе семинара Гусева Ильи на кафедре компьютерной лингвистики ABBYY МФТИ.`\n\nЗагрузим датасет и необходимые библиотеки","metadata":{"id":"Kmb8UhIzOnfK"}},{"cell_type":"code","source":"!wget -q https://www.dropbox.com/s/43l702z5a5i2w8j/gazeta_train.txt\n!wget -q https://www.dropbox.com/s/k2egt3sug0hb185/gazeta_val.txt\n!wget -q https://www.dropbox.com/s/3gki5n5djs9w0v6/gazeta_test.txt","metadata":{"id":"OqkLTkFRfXvA","execution":{"iopub.status.busy":"2022-08-02T16:34:08.915461Z","iopub.execute_input":"2022-08-02T16:34:08.916056Z","iopub.status.idle":"2022-08-02T16:34:24.323133Z","shell.execute_reply.started":"2022-08-02T16:34:08.915915Z","shell.execute_reply":"2022-08-02T16:34:24.321042Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip -q install razdel networkx pymorphy2 nltk rouge==0.3.1 summa youtokentome","metadata":{"id":"SXS1sdYZCluU","execution":{"iopub.status.busy":"2022-08-02T16:34:24.326002Z","iopub.execute_input":"2022-08-02T16:34:24.326609Z","iopub.status.idle":"2022-08-02T16:34:38.022803Z","shell.execute_reply.started":"2022-08-02T16:34:24.326553Z","shell.execute_reply":"2022-08-02T16:34:38.021448Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{"id":"Wa0NfryxbPUP"}},{"cell_type":"markdown","source":"Посмотрим на то, как устроен датасет","metadata":{"id":"eesnclfDDV3F"}},{"cell_type":"code","source":"!head -n 1 gazeta_train.txt\n!cat gazeta_train.txt | wc -l\n!cat gazeta_val.txt | wc -l\n!cat gazeta_test.txt | wc -l","metadata":{"id":"Mz6CZYKQhnd-","execution":{"iopub.status.busy":"2022-08-02T16:34:38.024763Z","iopub.execute_input":"2022-08-02T16:34:38.025345Z","iopub.status.idle":"2022-08-02T16:34:46.015043Z","shell.execute_reply.started":"2022-08-02T16:34:38.025299Z","shell.execute_reply":"2022-08-02T16:34:46.013253Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import json\nimport random\n\ndef read_gazeta_records(file_name, shuffle=True, sort_by_date=False):\n    assert shuffle != sort_by_date\n    records = []\n    with open(file_name, \"r\") as r:\n        for line in r:\n            records.append(json.loads(line))\n    if sort_by_date:\n        records.sort(key=lambda x: x[\"date\"])\n    if shuffle:\n        random.shuffle\n    return records","metadata":{"id":"5pZ2UGS2DGjH","execution":{"iopub.status.busy":"2022-08-02T16:34:46.019765Z","iopub.execute_input":"2022-08-02T16:34:46.020228Z","iopub.status.idle":"2022-08-02T16:34:46.029911Z","shell.execute_reply.started":"2022-08-02T16:34:46.020184Z","shell.execute_reply":"2022-08-02T16:34:46.028871Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_records = read_gazeta_records(\"gazeta_train.txt\")\nval_records = read_gazeta_records(\"gazeta_val.txt\")\ntest_records = read_gazeta_records(\"gazeta_test.txt\")","metadata":{"id":"GNDp-BunEA91","execution":{"iopub.status.busy":"2022-08-02T16:34:46.031513Z","iopub.execute_input":"2022-08-02T16:34:46.031932Z","iopub.status.idle":"2022-08-02T16:34:50.409888Z","shell.execute_reply.started":"2022-08-02T16:34:46.031894Z","shell.execute_reply":"2022-08-02T16:34:50.408048Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\nfrom rouge import Rouge\n\ndef calc_scores(references, predictions, metric=\"all\"):\n    print(\"Count:\", len(predictions))\n    print(\"Ref:\", references[-1])\n    print(\"Hyp:\", predictions[-1])\n\n    if metric in (\"bleu\", \"all\"):\n        print(\"BLEU: \", corpus_bleu([[r] for r in references], predictions))\n    if metric in (\"rouge\", \"all\"):\n        rouge = Rouge()\n        scores = rouge.get_scores(predictions, references, avg=True)\n        print(\"ROUGE: \", scores)","metadata":{"id":"397gjsNfFBZ_","execution":{"iopub.status.busy":"2022-08-02T16:34:50.411962Z","iopub.execute_input":"2022-08-02T16:34:50.413891Z","iopub.status.idle":"2022-08-02T16:34:52.592633Z","shell.execute_reply.started":"2022-08-02T16:34:50.413848Z","shell.execute_reply":"2022-08-02T16:34:52.591124Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Extractive RNN","metadata":{"id":"aaJKNsUGFBaA"}},{"cell_type":"markdown","source":"### BPE\nДля начала сделаем BPE токенизацию","metadata":{"id":"l3izlm8HFBaC"}},{"cell_type":"code","source":"import youtokentome as yttm\n\ndef train_bpe(records, model_path, model_type=\"bpe\", vocab_size=10000, lower=True):\n    temp_file_name = \"temp.txt\"\n    with open(temp_file_name, \"w\") as temp:\n        for record in records:\n            text, summary = record['text'], record['summary']\n            if lower:\n                summary = summary.lower()\n                text = text.lower()\n            if not text or not summary:\n                continue\n            temp.write(text + \"\\n\")\n            temp.write(summary + \"\\n\")\n    yttm.BPE.train(data=temp_file_name, vocab_size=vocab_size, model=model_path)\n\ntrain_bpe(train_records, \"BPE_model.bin\")","metadata":{"id":"Yg9T6q0wFBaF","execution":{"iopub.status.busy":"2022-08-02T16:34:52.594226Z","iopub.execute_input":"2022-08-02T16:34:52.594679Z","iopub.status.idle":"2022-08-02T16:35:11.918949Z","shell.execute_reply.started":"2022-08-02T16:34:52.594641Z","shell.execute_reply":"2022-08-02T16:35:11.917690Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Словарь\nСоставим словарь для индексации токенов","metadata":{"id":"jJFAJHTtFBaF"}},{"cell_type":"code","source":"bpe_processor = yttm.BPE('BPE_model.bin')\nvocabulary = bpe_processor.vocab()","metadata":{"id":"MueXtatmFBaG","execution":{"iopub.status.busy":"2022-08-02T16:35:11.923586Z","iopub.execute_input":"2022-08-02T16:35:11.924008Z","iopub.status.idle":"2022-08-02T16:35:11.971155Z","shell.execute_reply.started":"2022-08-02T16:35:11.923973Z","shell.execute_reply":"2022-08-02T16:35:11.969377Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Кэш oracle summary\nЗакэшируем oracle summary, чтобы не пересчитывать их каждый раз","metadata":{"id":"Z_C_p7tHFBaH"}},{"cell_type":"code","source":"from rouge import Rouge\nimport razdel\nfrom tqdm.notebook import tqdm\n\nimport copy\n\ndef build_oracle_summary_greedy(text, gold_summary, calc_score, lower=True, max_sentences=30):\n    '''\n    Жадное построение oracle summary\n    '''\n    gold_summary = gold_summary.lower() if lower else gold_summary\n    # Делим текст на предложения\n    sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n    n_sentences = len(sentences)\n    oracle_summary_sentences = set()\n    \n    score = -1.0\n    summaries = []\n    for _ in range(n_sentences):\n        for i in range(n_sentences):\n            if i in oracle_summary_sentences:\n                continue\n            current_summary_sentences = copy.copy(oracle_summary_sentences)\n            # Добавляем какое-то предложения к уже существующему summary\n            current_summary_sentences.add(i)\n            current_summary = \" \".join([sentences[index] for index in sorted(list(current_summary_sentences))])\n            # Считаем метрики\n            current_score = calc_score(current_summary, gold_summary)\n            summaries.append((current_score, current_summary_sentences))\n        # Если получилось улучшить метрики с добавлением какого-либо предложения, то пробуем добавить ещё\n        # Иначе на этом заканчиваем\n        best_summary_score, best_summary_sentences = max(summaries)\n        if best_summary_score <= score:\n            break\n        oracle_summary_sentences = best_summary_sentences\n        score = best_summary_score\n    oracle_summary = \" \".join([sentences[index] for index in sorted(list(oracle_summary_sentences))])\n    return oracle_summary, oracle_summary_sentences\n\ndef calc_single_score(pred_summary, gold_summary, rouge):\n    return rouge.get_scores([pred_summary], [gold_summary], avg=True)['rouge-2']['f']\n\ndef add_oracle_summary_to_records(records, max_sentences=30, lower=True, nrows=1000):\n    rouge = Rouge()\n    for i, record in tqdm(enumerate(records)):\n        if i >= nrows:\n            break\n        text = record[\"text\"]\n        summary = record[\"summary\"]\n\n        summary = summary.lower() if lower else summary\n        sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n        oracle_summary, sentences_indicies = build_oracle_summary_greedy(text, summary, calc_score=lambda x, y: calc_single_score(x, y, rouge),\n                                                                         lower=lower, max_sentences=max_sentences)\n        record[\"sentences\"] = sentences\n        record[\"oracle_sentences\"] = list(sentences_indicies)\n        record[\"oracle_summary\"] = oracle_summary\n\n    return records[:nrows]\n\next_train_records = add_oracle_summary_to_records(train_records, nrows=2048)\next_val_records = add_oracle_summary_to_records(val_records, nrows=256)\next_test_records = add_oracle_summary_to_records(test_records, nrows=256)","metadata":{"id":"Fp23tuPbFBaH","executionInfo":{"status":"ok","timestamp":1619439358923,"user_tz":-180,"elapsed":851,"user":{"displayName":"Deep Learning School","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNf0RkP5WvkU5MixKfC1Sv3mb-9QWgAbC6VcfQvA=s64","userId":"16549096980415837553"}},"execution":{"iopub.status.busy":"2022-08-02T16:35:11.973813Z","iopub.execute_input":"2022-08-02T16:35:11.974378Z","iopub.status.idle":"2022-08-02T16:43:39.069647Z","shell.execute_reply.started":"2022-08-02T16:35:11.974322Z","shell.execute_reply":"2022-08-02T16:43:39.068107Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Составление батчей","metadata":{"id":"UlXXc8qUHC5m"}},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"id":"YATQKCuqHPo3","execution":{"iopub.status.busy":"2022-08-02T16:43:39.075456Z","iopub.execute_input":"2022-08-02T16:43:39.076568Z","iopub.status.idle":"2022-08-02T16:43:41.254448Z","shell.execute_reply.started":"2022-08-02T16:43:39.076517Z","shell.execute_reply":"2022-08-02T16:43:41.253105Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import random\nimport math\nimport razdel\nimport torch\nimport numpy as np\nfrom rouge import Rouge\n\n\nclass BatchIterator():\n    def __init__(self, records, vocabulary, batch_size, bpe_processor, shuffle=True, lower=True, max_sentences=30, max_sentence_length=50, device=torch.device('cpu')):\n        self.records = records\n        self.num_samples = len(records)\n        self.batch_size = batch_size\n        self.bpe_processor = bpe_processor\n        self.shuffle = shuffle\n        self.batches_count = int(math.ceil(self.num_samples / batch_size))\n        self.lower = lower\n        self.rouge = Rouge()\n        self.vocabulary = vocabulary\n        self.max_sentences = max_sentences\n        self.max_sentence_length = max_sentence_length\n        self.device = device\n        \n    def __len__(self):\n        return self.batches_count\n    \n    def __iter__(self):\n        indices = np.arange(self.num_samples)\n        if self.shuffle:\n            np.random.shuffle(indices)\n\n        for start in range(0, self.num_samples, self.batch_size):\n            end = min(start + self.batch_size, self.num_samples)\n            batch_indices = indices[start:end]\n\n            batch_inputs = []\n            batch_outputs = []\n            max_sentence_length = 0\n            max_sentences = 0\n            batch_records = []\n\n            for data_ind in batch_indices:\n                \n                record = self.records[data_ind]\n                batch_records.append(record)\n                text = record[\"text\"]\n                summary = record[\"summary\"]\n                summary = summary.lower() if self.lower else summary\n\n                if \"sentences\" not in record:\n                    sentences = [sentence.text.lower() if self.lower else sentence.text for sentence in razdel.sentenize(text)][:self.max_sentences]\n                else:\n                    sentences = record[\"sentences\"]\n                max_sentences = max(len(sentences), max_sentences)\n                \n                # номера предложений, которые в нашем саммари\n                if \"oracle_sentences\" not in record:\n                    calc_score = lambda x, y: calc_single_score(x, y, self.rouge)\n                    sentences_indicies = build_oracle_summary_greedy(text, summary, calc_score=calc_score, lower=self.lower, max_sentences=self.max_sentences)[1]\n                else:   \n                    sentences_indicies = record[\"oracle_sentences\"]\n                \n                # inputs - индексы слов в предложении\n                inputs = [bpe_processor.encode(sentence)[:self.max_sentence_length] for sentence in sentences]\n                max_sentence_length = max(max_sentence_length, max([len(tokens) for tokens in inputs]))\n                \n                # получение метки класса предложения\n                outputs = [int(i in sentences_indicies) for i in range(len(sentences))]\n                batch_inputs.append(inputs)\n                batch_outputs.append(outputs)\n\n            tensor_inputs = torch.zeros((self.batch_size, max_sentences, max_sentence_length), dtype=torch.long, device=self.device)\n            # we add index 2 for padding\n            # YOUR CODE\n            tensor_outputs = torch.zeros((self.batch_size, max_sentences), dtype=torch.float32, device=self.device)\n\n            for i, inputs in enumerate(batch_inputs):\n                for j, sentence_tokens in enumerate(inputs):\n                    tensor_inputs[i][j][:len(sentence_tokens)] = torch.tensor(sentence_tokens, dtype=torch.int64)\n                    tensor_inputs[i][j][len(sentence_tokens):] = torch.tensor(2, dtype=torch.int64)\n\n            for i, outputs in enumerate(batch_outputs):\n                tensor_outputs[i][:len(outputs)] = torch.LongTensor(outputs)\n\n            tensor_outputs = tensor_outputs.long()\n            yield {\n                'inputs': tensor_inputs,\n                'outputs': tensor_outputs,\n                'records': batch_records\n            }","metadata":{"id":"MNyxstTChK3C","execution":{"iopub.status.busy":"2022-08-02T16:43:41.256937Z","iopub.execute_input":"2022-08-02T16:43:41.257716Z","iopub.status.idle":"2022-08-02T16:43:41.282518Z","shell.execute_reply.started":"2022-08-02T16:43:41.257671Z","shell.execute_reply":"2022-08-02T16:43:41.281389Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_iterator = BatchIterator(ext_train_records, vocabulary, 32, bpe_processor, device=device)\nval_iterator = BatchIterator(ext_val_records, vocabulary, 32, bpe_processor, device=device)\ntest_iterator = BatchIterator(ext_test_records, vocabulary, 32, bpe_processor, device=device)","metadata":{"id":"5ug9MIObdi03","execution":{"iopub.status.busy":"2022-08-02T16:43:41.284511Z","iopub.execute_input":"2022-08-02T16:43:41.285234Z","iopub.status.idle":"2022-08-02T16:43:41.297910Z","shell.execute_reply.started":"2022-08-02T16:43:41.285194Z","shell.execute_reply":"2022-08-02T16:43:41.296602Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extractor -  SummaRuNNer\n https://arxiv.org/pdf/1611.04230.pdf\n","metadata":{"id":"yPlJMg0_dQM-"}},{"cell_type":"markdown","source":"### Homework\n\n* [x] В данной реализации в `outputs` в качестве padding используется индекс 0. Измените в функции `__iter__` индекс padding, чтобы он не совпадал с классом 0 или 1, например, 2.\n* [x] В качестве criterion используйте `CrossEntropyLoss`вместо `BCEWithLogitsLoss`\n* Из-за смены criterion, вы уже должны подавать на вход criterion ни одно число, а logits для каждого класса. Перед подачей logits вы можете отфильтровать предсказания для класса padding. В этом пункте вам придется изменять файл `train_model.py`, а именно функциии `train` и `evaluate`.\n* Используйте два варианта обучения: c весами в `CrossEntropyLoss` и без\n* Также сравните `inference`, когда вы ранжируете logits, и когды вы выбирате предложения, у котрых logits > 0, в двух вариантах обучения. \n* Реализуйте дополнительно характеристику предложения `novelty`. Как влияет добавление `novelty` на качество summary?\n* Постарайтесь улучшить качество модели, полученной на семинаре: $BLEU \\approx 0.45$","metadata":{"id":"4BSsnfe4t1uK"}},{"cell_type":"code","source":"import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nfrom torch.nn.utils.rnn import pack_padded_sequence as pack\nfrom torch.nn.utils.rnn import pad_packed_sequence as unpack\n\nclass SentenceEncoderRNN(nn.Module):\n    def __init__(self, input_size, embedding_dim, hidden_size, n_layers=3, dropout=0.3, bidirectional=True):\n        super().__init__()\n\n        num_directions = 2 if bidirectional else 1\n        assert hidden_size % num_directions == 0\n        hidden_size = hidden_size // num_directions\n\n        self.embedding_dim = embedding_dim\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n\n        self.embedding_layer = nn.Embedding(input_size, embedding_dim)\n        self.rnn_layer = nn.LSTM(embedding_dim, hidden_size, n_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n        self.dropout_layer = nn.Dropout(dropout)\n\n    def forward(self, inputs, hidden=None):\n        embedded = self.dropout_layer(self.embedding_layer(inputs))\n        outputs, _ = self.rnn_layer(embedded, hidden)\n        sentences_embeddings = torch.mean(outputs, 1)\n        # [batch_size, hidden_size]\n        return sentences_embeddings\n\nclass SentenceTaggerRNN(nn.Module):\n    def __init__(self,\n                 vocabulary_size,\n                 use_content=True,\n                 use_salience=True,\n                 use_novelty=True,\n                 token_embedding_dim=128,\n                 sentence_encoder_hidden_size=256,\n                 hidden_size=256,\n                 bidirectional=True,\n                 sentence_encoder_n_layers=2,\n                 sentence_encoder_dropout=0.3,\n                 sentence_encoder_bidirectional=True,\n                 n_layers=2,\n                 dropout=0.3):\n        \n        super().__init__()\n\n        num_directions = 2 if bidirectional else 1\n        assert hidden_size % num_directions == 0\n        hidden_size = hidden_size // num_directions\n\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n\n        self.sentence_encoder = SentenceEncoderRNN(vocabulary_size, token_embedding_dim,\n                                                   sentence_encoder_hidden_size, sentence_encoder_n_layers, \n                                                   sentence_encoder_dropout, sentence_encoder_bidirectional)\n        \n        self.rnn_layer = nn.LSTM(sentence_encoder_hidden_size, hidden_size, n_layers, dropout=dropout,\n                           bidirectional=bidirectional, batch_first=True)\n        \n        self.dropout_layer = nn.Dropout(dropout)\n        self.content_linear_layer = nn.Linear(hidden_size * 2, 1)\n        self.document_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n        self.salience_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n        self.novelty_linear_layer =  nn.Linear(hidden_size * 2, hidden_size * 2) # nn.Linear(hidden_size * 2, 1) # YOUR CODE\n        self.tanh_layer = nn.Tanh()\n\n        self.use_content = use_content\n        self.use_salience = use_salience\n        self.use_novelty = use_novelty\n\n    def forward(self, inputs, hidden=None):\n        # parameters of the probability\n        content = 0\n        salience = 0\n        novelty = 0\n\n        # [batch_size, seq num, seq_len]\n        batch_size = inputs.size(0)\n        sentences_count = inputs.size(1)\n        tokens_count = inputs.size(2)\n        inputs = inputs.reshape(-1, tokens_count)\n        # [batch_size * seq num, seq_len]\n\n        embedded_sentences = self.sentence_encoder(inputs)\n        embedded_sentences = self.dropout_layer(embedded_sentences.reshape(batch_size, sentences_count, -1))\n        # [batch_size *  seq num, seq_len, hidden_size] -> [batch_size, seq num, hidden_size]\n\n        outputs, _ = self.rnn_layer(embedded_sentences, hidden)\n        # [batch_size, seq num, hidden_size]\n\n        document_embedding = self.tanh_layer(self.document_linear_layer(torch.mean(outputs, 1)))\n        # [batch_size, hidden_size]\n\n        # W * h^T\n        if self.use_content:\n            content = self.content_linear_layer(outputs).squeeze(2) # 1-representation\n        # [batch_size, seq num]\n\n        # h^T * W * d\n        if self.use_salience:\n            salience = torch.bmm(outputs, self.salience_linear_layer(document_embedding).unsqueeze(2)).squeeze(2) # 2-representation\n        # [batch_size, seq num, hidden_size] * [batch_size, hidden_size, 1] = [batch_size, seq num, ]\n\n        if self.use_novelty:\n            # at every step add novelty to prediction of the sentence\n            predictions = content + salience\n            print(predictions.size())\n            # 0) initialize summary_representation and novelty by zeros\n            # YOUR CODE\n            summary_representation = torch.zeros(outputs.shape).to(device) # [batch_size, seq num, hidden_size]\n            novelty = torch.zeros(batch_size, sentences_count) # [batch_size, seq num, ]\n\n            for sentence_num in range(sentences_count):\n                \n                # 1) take sentence_num_state from outputs(representation of the sentence with number sentence_num)\n                sentence_num_state = outputs[:, sentence_num, :]\n                \n                print('sentence_num_state:', sentence_num_state.shape)\n                print('outputs', outputs[:, sentence_num, :].size())\n                \n                # 2) calculate novelty for current sentence\n#                 wh_t = self.novelty_linear_layer(F.tanh(sentence_num_state)).unsqueeze(2)\n                wh_t = self.novelty_linear_layer(torch.tanh(summary_representation[:, sentence_num, :])).unsqueeze(2)\n                print('wh_t', wh_t.size())\n                novelty[:, sentence_num] = torch.bmm(sentence_num_state, wh_t).squeeze(2)\n                print('novelty', novelty.size())\n                \n                # 3) add novelty to predictions\n                predictions += novelty\n                \n                \n                \n                # 4) calculcate probability for current sentence\n                # 5) add sentence_num_state with the weight which is equal to probability to summary_representation\n\n                # YOUR CODE\n\n        return content + salience + novelty","metadata":{"execution":{"iopub.status.busy":"2022-08-02T16:43:41.300089Z","iopub.execute_input":"2022-08-02T16:43:41.300613Z","iopub.status.idle":"2022-08-02T16:43:41.330627Z","shell.execute_reply.started":"2022-08-02T16:43:41.300569Z","shell.execute_reply":"2022-08-02T16:43:41.329560Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nfrom torch.nn.utils.rnn import pack_padded_sequence as pack\nfrom torch.nn.utils.rnn import pad_packed_sequence as unpack\n\nclass SentenceEncoderRNN(nn.Module):\n    def __init__(self, input_size, embedding_dim, hidden_size, n_layers=3, dropout=0.3, bidirectional=True):\n        super().__init__()\n\n        num_directions = 2 if bidirectional else 1\n        assert hidden_size % num_directions == 0\n        hidden_size = hidden_size // num_directions\n\n        self.embedding_dim = embedding_dim\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n\n        self.embedding_layer = nn.Embedding(input_size, embedding_dim)\n        self.rnn_layer = nn.LSTM(embedding_dim, hidden_size, n_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n        self.dropout_layer = nn.Dropout(dropout)\n\n    def forward(self, inputs, hidden=None):\n        embedded = self.dropout_layer(self.embedding_layer(inputs))\n        outputs, _ = self.rnn_layer(embedded, hidden)\n        sentences_embeddings = torch.mean(outputs, 1)\n        # [batch_size, hidden_size]\n        return sentences_embeddings\n\nclass SentenceTaggerRNN(nn.Module):\n    def __init__(self,\n                 vocabulary_size,\n                 use_content=True,\n                 use_salience=True,\n                 use_novelty=True,\n                 token_embedding_dim=128,\n                 sentence_encoder_hidden_size=256,\n                 hidden_size=256,\n                 bidirectional=True,\n                 sentence_encoder_n_layers=2,\n                 sentence_encoder_dropout=0.3,\n                 sentence_encoder_bidirectional=True,\n                 n_layers=2,\n                 dropout=0.3):\n        \n        super().__init__()\n\n        num_directions = 2 if bidirectional else 1\n        assert hidden_size % num_directions == 0\n        hidden_size = hidden_size // num_directions\n\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n\n        self.sentence_encoder = SentenceEncoderRNN(vocabulary_size, token_embedding_dim,\n                                                   sentence_encoder_hidden_size, sentence_encoder_n_layers, \n                                                   sentence_encoder_dropout, sentence_encoder_bidirectional)\n        \n        self.rnn_layer = nn.LSTM(sentence_encoder_hidden_size, hidden_size, n_layers, dropout=dropout,\n                           bidirectional=bidirectional, batch_first=True)\n        \n        self.dropout_layer = nn.Dropout(dropout)\n        self.content_linear_layer = nn.Linear(hidden_size * 2, 1)\n        self.document_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n        self.salience_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n        self.novelty_linear_layer =  nn.Linear(hidden_size * 2, hidden_size * 2) # nn.Linear(hidden_size * 2, 1) # YOUR CODE\n        self.tanh_layer = nn.Tanh()\n\n        self.use_content = use_content\n        self.use_salience = use_salience\n        self.use_novelty = use_novelty\n\n    def forward(self, inputs, hidden=None):\n        # parameters of the probability\n        content = 0\n        salience = 0\n        novelty = 0\n\n        # [batch_size, seq num, seq_len]\n        batch_size = inputs.size(0)\n        sentences_count = inputs.size(1)\n        tokens_count = inputs.size(2)\n        inputs = inputs.reshape(-1, tokens_count)\n        # [batch_size * seq num, seq_len]\n\n        embedded_sentences = self.sentence_encoder(inputs)\n        embedded_sentences = self.dropout_layer(embedded_sentences.reshape(batch_size, sentences_count, -1))\n        # [batch_size *  seq num, seq_len, hidden_size] -> [batch_size, seq num, hidden_size]\n\n        outputs, _ = self.rnn_layer(embedded_sentences, hidden)\n        # [batch_size, seq num, hidden_size]\n\n        document_embedding = self.tanh_layer(self.document_linear_layer(torch.mean(outputs, 1)))\n        # [batch_size, hidden_size]\n\n        # W * h^T\n        if self.use_content:\n            content = self.content_linear_layer(outputs).squeeze(2) # 1-representation\n        # [batch_size, seq num]\n\n        # h^T * W * d\n        if self.use_salience:\n            salience = torch.bmm(outputs, self.salience_linear_layer(document_embedding).unsqueeze(2)).squeeze(2) # 2-representation\n        # [batch_size, seq num, hidden_size] * [batch_size, hidden_size, 1] = [batch_size, seq num, ]\n\n        if self.use_novelty:\n            # at every step add novelty to prediction of the sentence\n            predictions = content + salience\n            print(predictions.size())\n            # 0) initialize summary_representation and novelty by zeros\n            # YOUR CODE\n            summary_representation = torch.zeros(outputs.size()) # [batch_size, seq num, ]\n            novelty = torch.zeros(batch_size, sentences_count) # [batch_size, seq num, ]\n            probabilities = torch.zeros((batch_size, sentences_count)).to(device) # [batch_size, seq num, ]\n\n            for sentence_num in range(sentences_count):\n                \n                # 1) take sentence_num_state from outputs(representation of the sentence with number sentence_num)\n                sentence_num_state = outputs[:, sentence_num, :]\n                \n#                 print('sentence_num_state:', sentence_num_state.shape)\n#                 print('outputs', outputs[:, sentence_num, :].size())\n                \n                # 2) calculate novelty for current sentence\n                wh_t = self.novelty_linear_layer(torch.tanh(sentence_num_state)).unsqueeze(2)\n#                 print('wh_t', wh_t.size())\n                novelty[:, sentence_num] = torch.bmm(outputs, wh_t).squeeze(2)[:, sentence_num]\n#                 print('novelty', novelty.size())\n                \n                # 3) add novelty to predictions\n                preds = predictions[:, sentence_num].clone() + novelty[:, sentence_num].clone()\n#                 probabilities[:, sentence_num] = torch.sigmoid(predictions)\n                probabilities[:, sentence_num] = torch.sigmoid(preds)\n#                 print('probabilities', probabilities.size())\n                summary_representation[:, sentence_num, :] = (outputs[:, :sentence_num, :].clone() * probabilities[:, :sentence_num].unsqueeze(2).clone()).sum(dim=1)\n                \n                # 4) calculcate probability for current sentence\n                # 5) add sentence_num_state with the weight which is equal to probability to summary_representation\n\n                # YOUR CODE\n        result = predictions - novelty\n        result = torch.cat([-result.unsqueeze(1), result.unsqueeze(1)], dim=1) # for cross entropy loss\n        return result","metadata":{"id":"iW7iS76KeEdO","execution":{"iopub.status.busy":"2022-08-02T17:13:01.479023Z","iopub.execute_input":"2022-08-02T17:13:01.479558Z","iopub.status.idle":"2022-08-02T17:13:01.514854Z","shell.execute_reply.started":"2022-08-02T17:13:01.479516Z","shell.execute_reply":"2022-08-02T17:13:01.513408Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"inputs = next(iter(train_iterator))['inputs']\nvocab_size = len(vocabulary)\nmodel = SentenceTaggerRNN(vocab_size).to(device)\n\noutputs = model(inputs.to(device))","metadata":{"execution":{"iopub.status.busy":"2022-08-02T17:13:06.641690Z","iopub.execute_input":"2022-08-02T17:13:06.642151Z","iopub.status.idle":"2022-08-02T17:13:09.056642Z","shell.execute_reply.started":"2022-08-02T17:13:06.642113Z","shell.execute_reply":"2022-08-02T17:13:09.055202Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Model\n$P\\left(y_{j} = 1 \\mid \\mathbf{h}_{j}, \\mathbf{s}_{j}, \\mathbf{d}\\right)=\\sigma\\left(W_{c} \\mathbf{h}_{j} + \\mathbf{h}_{j}^{T} W_{s} \\mathbf{d}\\right)$\n--------------------","metadata":{"id":"QxpL3AtOrctD"}},{"cell_type":"code","source":"vocab_size = len(vocabulary)\nmodel = SentenceTaggerRNN(vocab_size).to(device)\n\nparams_count = np.sum([p.numel() for p in model.parameters() if p.requires_grad])\nprint(\"Trainable params: {}\".format(params_count))","metadata":{"id":"-QC1ZmuQfB7f","execution":{"iopub.status.busy":"2022-08-02T16:43:44.079089Z","iopub.status.idle":"2022-08-02T16:43:44.079616Z","shell.execute_reply.started":"2022-08-02T16:43:44.079388Z","shell.execute_reply":"2022-08-02T16:43:44.079412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(f\"{name}: {param.numel()}\")","metadata":{"id":"f9Q3aFHhgsB4","execution":{"iopub.status.busy":"2022-08-02T16:43:44.081873Z","iopub.status.idle":"2022-08-02T16:43:44.082666Z","shell.execute_reply.started":"2022-08-02T16:43:44.082208Z","shell.execute_reply":"2022-08-02T16:43:44.082241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown https://drive.google.com/uc?id=1MiS_iczALcyF7zGDPY6niyeD82P0_PBH -O train_model.py\nimport train_model\nimport imp \nimp.reload(train_model)\nfrom train_model import train_with_logs","metadata":{"id":"05s8UWh81cjG","execution":{"iopub.status.busy":"2022-08-02T16:43:44.085759Z","iopub.status.idle":"2022-08-02T16:43:44.087211Z","shell.execute_reply.started":"2022-08-02T16:43:44.086689Z","shell.execute_reply":"2022-08-02T16:43:44.086729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 3\nCLIP = 1\n\ndef train(use_class_weights, N_EPOCHS, CLIP, lr=1e-3):\n    optimizer = optim.Adam(model.parameters(), lr)\n    if use_class_weights:\n        # weights depend on the number of objects of class 0 and 1\n        # YOUR CODE\n        criterion = nn.CrossEntropyLoss(weight=weights)\n    else:\n        criterion = nn.CrossEntropyLoss()\n    train_with_logs(model, train_iterator, val_iterator, optimizer, criterion, N_EPOCHS, CLIP)\n\ntrain(True, N_EPOCHS, CLIP)","metadata":{"id":"rwrhG4v71yts","execution":{"iopub.status.busy":"2022-08-02T16:43:44.090103Z","iopub.status.idle":"2022-08-02T16:43:44.090953Z","shell.execute_reply.started":"2022-08-02T16:43:44.090716Z","shell.execute_reply":"2022-08-02T16:43:44.090742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"vuv4Sh2Vj5Yr"}},{"cell_type":"code","source":"from train_model import punct_detokenize, postprocess\n\ndef inference_summarunner(model, iterator, top_k=3):\n\n    references = []\n    predictions = []\n\n    model.eval()\n    for batch in test_iterator:\n\n        logits = model(batch['inputs'])\n        sum_in = torch.argsort(logits, dim=1)[:, -top_k:]\n        \n        for i in range(len(batch['outputs'])):\n\n            summary = batch['records'][i]['summary'].lower()\n            pred_summary = ' '.join([batch['records'][i]['sentences'][ind] for ind in sum_in.sort(dim=1)[0][i]])\n\n            summary, pred_summary = postprocess(summary, pred_summary)\n\n            references.append(summary)\n            predictions.append(pred_summary)\n\n    calc_scores(references, predictions)\n\nmodel.load_state_dict(torch.load('best-val-model.pt'))\ninference_summarunner(model, test_iterator, 3)","metadata":{"id":"wZ96X37bb_PV","execution":{"iopub.status.busy":"2022-08-02T16:43:44.092244Z","iopub.status.idle":"2022-08-02T16:43:44.092698Z","shell.execute_reply.started":"2022-08-02T16:43:44.092476Z","shell.execute_reply":"2022-08-02T16:43:44.092497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Вывод:","metadata":{"id":"HGc4IV8dzyBP"}}]}